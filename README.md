# User Feedback Sentiment (BigQuery + GPT-4o)

This project analyzes structured user feedback (thumbs up/down, flags, and comments) using a fine-grained LLM-based sentiment scoring system. It processes data from a BigQuery table, generates numerical sentiment scores and aspect labels via OpenAIâ€™s GPT-4o, and stores the results back in BigQuery for visualization and monitoring.

## ğŸ’¡ Features

- ğŸ” Classifies sentiment as positive or negative with **numerical intensity from -2 to +2**
- ğŸ§  Uses GPT-4o to analyze comment tone and identify specific aspects being addressed
- ğŸ“Š Saves structured output to a BigQuery table for downstream use in dashboards (e.g. Metabase)
- ğŸ” Designed for batch re-processing and easy automation via scheduled queries or Cloud Functions

## ğŸ› ï¸ Stack

- Python 3.11
- OpenAI GPT-4o API
- Google BigQuery
- `.env` for secret management (for local testing only--prod version incorporates Google Secret manager)
- Optional: Metabase (for visualizations), Slack (for alerts)

## ğŸ—‚ï¸ Project Structure

```
user-feedback-sentiment-bq/
â”œâ”€â”€ test_llm_pipeline.py       # Test script to run on sample CSV data
â”œâ”€â”€ main_llm_pipeline.py       # Production script for full BigQuery dataset
â”œâ”€â”€ prompts.py                 # Modular prompt templates for GPT-4o
â”œâ”€â”€ utils.py                   # Utilities for API calls, error handling, etc.
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example               # Example environment variable setup
```

## âš™ï¸ How It Works

1. Query feedback from BigQuery (e.g. thumbs up/down with free-text comments)
2. For each row, send a prompt to GPT-4o to:
   - Assign a sentiment score from -2 to +2
   - Extract the topic or aspect of the original system message being commented on
3. Return structured JSON output
4. Insert results back into a BigQuery output table (e.g. `Model.feedback_sentiment_output`)

## ğŸ§ª Running the Pipeline Locally

1. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up your `.env`**:
   ```
   OPENAI_API_KEY=your-key-here
   BIGQUERY_PROJECT=your-gcp-project
   BIGQUERY_DATASET=your-dataset
   ```

3. **Run the test script** (20-row sample):
   ```bash
   python test_llm_pipeline.py
   ```

4. **Run full BigQuery pipeline**:
   ```bash
   python main_llm_pipeline.py
   ```

## ğŸ“ˆ Example Output Schema

| feedback_id | sentiment_score | sentiment_label     | comment_aspect       | model_used |
|-------------|------------------|----------------------|-----------------------|-------------|
| 12345       | -2               | strongly negative    | message clarity       | gpt-4o      |
| 12346       | +1               | slightly positive    | system tone           | gpt-4o      |

## ğŸ”® Future Plans

- Add support for multilingual feedback
- Hook into Slack or email alerts on extreme negative feedback
- Compare LLM model performance (Claude vs GPT-4o)
- Export labeled data for model fine-tuning

## ğŸ“„ License

MIT License. Feel free to use and adapt this pipeline for your own feedback analysis workflows.

---

ğŸ§  Built with care to bring nuance to user sentiment.
